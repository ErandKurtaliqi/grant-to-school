# Year-wise Grant to School Students of Class (6â€“10)

**Dataset Source:** [Kaggle â€“ Year-wise Grant to School Students of Class (6â€“10)](https://www.kaggle.com/datasets/xixama/year-wise-grant-to-school-students-of-class-6-10)

---

## Project Overview

This project focuses on the **preprocessing and preparation of educational grant data** for students in classes 6â€“10.  
The dataset provides insights into year-wise grants distributed to school students, serving as a foundation for future data analysis and machine learning tasks.

In this phase, we have performed **comprehensive data preprocessing** to ensure data quality, integrity, and readiness for analytical modeling.

#### **ğŸ” Output After Data Cleaning (First 5 Rows)**

| Year | Student ID | State | District | Aspirational | Category | Class | Gender | Scholarship |
|------|------------|--------|-----------|--------------|----------|--------|---------|-------------|
| 2023 | 198164 | Punjab | Moga | Aspirational | Gen | 6 | Boys | 10000 |
| 2023 | 198165 | Rajasthan | Karauli | Aspirational | Gen | 6 | Boys | 10000 |
| 2023 | 198166 | Rajasthan | Sirohi | Aspirational | Gen | 6 | Boys | 10000 |
| 2023 | 198167 | Rajasthan | Sirohi | Aspirational | Gen | 7 | Boys | 10000 |
| 2023 | 198168 | Punjab | Firozpur | Aspirational | Gen | 8 | Boys | 10000 |

---

## Major Tasks in Data Preprocessing

### 1. Data Cleaning
We focused on improving data quality and consistency through:
- Filling in **missing values** using appropriate imputation techniques;  
- **Smoothing noisy data** to remove irregularities and inconsistencies;  
- Identifying and removing **outliers** that distort the dataset;  
- Resolving **inconsistencies** in naming, formatting, and categorical labels.

| Year | Student ID | State | District | Aspirational | Category | Class | Gender | Scholarship |
|------|------------|--------|-----------|--------------|----------|--------|---------|-------------|
| 2023 | 198164 | Punjab | Moga | Aspirational | Gen | 6 | Boys | 10000 |
| 2023 | 198165 | Rajasthan | Karauli | Aspirational | Gen | 6 | Boys | 10000 |
| 2023 | 198166 | Rajasthan | Sirohi | Aspirational | Gen | 6 | Boys | 10000 |
| 2023 | 198167 | Rajasthan | Sirohi | Aspirational | Gen | 7 | Boys | 10000 |
| 2023 | 198168 | Punjab | Firozpur | Aspirational | Gen | 8 | Boys | 10000 |


---

### 2. Data Reduction
To optimize performance and storage, we applied several data reduction techniques:
- **Dimensionality Reduction:** Removed redundant or less significant attributes;  
- **Numerosity Reduction:** Aggregated and summarized data to reduce record count while maintaining essential information;  
- **Data Compression:** Utilized encoding and compact formats to minimize data size.

| Year | Student ID | State | District | Aspirational | Category | Class | Gender | Scholarship |
|------|------------|--------|-----------|--------------|----------|--------|---------|-------------|
| 2023 | 198164 | Punjab | Moga | Aspirational | Gen | 6 | Boys | 10000 |
| 2023 | 198165 | Rajasthan | Karauli | Aspirational | Gen | 6 | Boys | 10000 |
| 2023 | 198166 | Rajasthan | Sirohi | Aspirational | Gen | 6 | Boys | 10000 |
| 2023 | 198167 | Rajasthan | Sirohi | Aspirational | Gen | 7 | Boys | 10000 |
| 2023 | 198168 | Punjab | Firozpur | Aspirational | Gen | 8 | Boys | 10000 |


---

### 3. Data Integration
We transformed and standardized data to make it more suitable for analysis:
- **Loaded the typed dataset from Phase 1 - Part 2
- **Optionally checked for a regional lookup file `State_Regions.csv` and merged it.
- **Verified the resulting schema and record count.

## Output

| Year | Student ID | State | District | Aspirational | Category | Class | Gender | Scholarship |
|------|------------|--------|-----------|--------------|----------|--------|---------|-------------|
| 2023 | 198164 | Punjab | Moga | Aspirational | Gen | 6 | Boys | 10000 |
| 2023 | 198165 | Rajasthan | Karauli | Aspirational | Gen | 6 | Boys | 10000 |
| 2023 | 198166 | Rajasthan | Sirohi | Aspirational | Gen | 6 | Boys | 10000 |
| 2023 | 198167 | Rajasthan | Sirohi | Aspirational | Gen | 7 | Boys | 10000 |
| 2023 | 198168 | Punjab | Firozpur | Aspirational | Gen | 8 | Boys | 10000 |



---

### 4. Data Aggregation
After preprocessing, **data aggregation** was performed to derive summarized views â€”  
combining records by year, class, and gender to identify overall trends and funding patterns.

## Output

| State Name | Gender | Category | Aspirational Final | Class | Total Scholarship |
|-------------|--------|----------|----------------------|--------|---------------------|
| Andaman and Nicobar Islands | Boys | Gen | Non-Aspirational | 6 | 10000.0 |
| Andaman and Nicobar Islands | Boys | Gen | Non-Aspirational | 7 | 10000.0 |
| Andaman and Nicobar Islands | Boys | Gen | Non-Aspirational | 8 | 10000.0 |
| Andaman and Nicobar Islands | Boys | Gen | Non-Aspirational | 9 | 10000.0 |
| Andaman and Nicobar Islands | Boys | Gen | Non-Aspirational | 10 | 10000.0 |


### 5. Binarization & Normalization
Final step that prepares data for modeling or analysis.

**Techniques applied:**
- Minâ€“Max normalization;  
- Binary encoding (True/False â†’ 1/0);  
- Feature scaling.

#### **ğŸ” Output After Binarization & Normalization (First 5 Rows)**

| State Name | Category | Aspirational Binary | Class Normalized | Scholarship | Gender Binary |
|-------------|----------|----------------------|-------------------|-------------|----------------|
| Andaman and Nicobar Islands | Gen | False | 0.00 | 10000.0 | True |
| Andaman and Nicobar Islands | Gen | False | 0.25 | 10000.0 | True |
| Andaman and Nicobar Islands | Gen | False | 0.50 | 10000.0 | True |
| Andaman and Nicobar Islands | Gen | False | 0.75 | 10000.0 | True |
| Andaman and Nicobar Islands | Gen | False | 1.00 | 10000.0 | True |


---

## Objectives
- Clean, structure, and prepare the dataset for future analysis;  
- Enable efficient visualization and trend identification;  
- Build a solid base for predictive modeling and grant distribution optimization.

---

## Repository Structure

This repository is organized into multiple folders, each representing a **specific phase** of data preprocessing.  
Every phase contains a `README.md` for documentation, a `.csv` dataset output, and a `.py` script used for that stage.

<img width="243" height="686" alt="image" src="https://github.com/user-attachments/assets/b050a7c5-745b-4b77-a70e-46fad55a8ef1" />
<img width="517" height="763" alt="image" src="https://github.com/user-attachments/assets/ea1bc8b4-a533-4fde-8a62-d44cf8add79e" />

---

## Phase Descriptions

### **1. Preprocessing for Analysis**
This folder contains the **first stage** of the pipeline.  
It ensures data consistency, removes errors, and prepares the dataset for later stages.

**Includes:**
- `script_preprocessing.py` â€“ performs initial cleaning, encoding, and column formatting.  
- `dataset_preprocessed.csv` â€“ cleaned dataset ready for data typing.  
- `README.md` â€“ describes preprocessing methods and reasoning.

**Goals:**
- Remove duplicates and nulls;  
- Standardize text fields;  
- Ensure consistent column names and formats.

---

### **2. Data Collection & Type Definition**
This step defines and validates the **data schema and types**.  

**Includes:**
- `script_data_collection.py` â€“ ensures every column has the correct type (numeric, categorical, etc.);  
- `dataset_typed_quality_checked.csv` â€“ dataset after type correction and validation;  
- `README.md` â€“ documents data typing and validation rules.

**Focus:**  
- Type conversions (string â†’ int/float);  
- Detection of invalid values;  
- Structural validation of columns.

---

### **3. Integration, Aggregation & Cleaning**
This is a **multi-part phase** responsible for unifying, cleaning, and summarizing the dataset.

#### *Integration*
Merges multiple sources or datasets into a single consistent dataset.  
- Aligns schemas;  
- Removes redundancy;  
- Produces `dataset_integrated.csv`.

#### *Cleaning_MissingValues*
Handles missing and incomplete data using:
- Mean/median imputation;  
- Mode replacement for categories;  
- Removal of records with excessive missing values.  
Produces `dataset_cleaned.csv`.

#### *Aggregation*
Performs grouped aggregations by year, class, or gender.  
Summarizes total and average grant distributions.  
Produces `dataset_aggregated.csv`.

---

### **4. Binarization & Normalization**
Final step that prepares data for modeling or analysis.

**Includes:**
- `script_binarization_normalization.py` â€“ converts categorical data to binary (0/1) and normalizes numeric features;  
- `final_binarization_file.csv` â€“ standardized dataset ready for analysis;  
- `README.md` â€“ explains binarization and normalization methods.

**Techniques applied:**
- Minâ€“Max normalization;  
- One-hot encoding;  
- Feature scaling for comparability.

---

## Summary Table

| Phase | Folder | Task | Output |
|-------|---------|------|--------|
| 1 | `1_Preprocessing_for_Analysis` | Cleaning, formatting | `dataset_preprocessed.csv` |
| 2 | `2_Data_Collection_Type_Definition` | Type validation | `dataset_typed_quality_checked.csv` |
| 3 | `3_Integration_Aggregation_Cleaning` | Integration, cleaning, aggregation | `dataset_integrated.csv`, `dataset_cleaned.csv`, `dataset_aggregated.csv` |
| 4 | `4_Binarization_Normalization` | Normalization and Binarization | `final_binarization_file.csv` |

---

## Key Takeaways
- A clear **four-phase pipeline** for data preparation;  
- Clean and modular Python scripts for each transformation step;  
- Progressive improvement in **data quality and usability**;  
- Ready-to-analyze dataset suitable for statistical or ML-based exploration.

# Skripti pÃ«r Detektimin e PÃ«rjashtuesve (Outliers) me IQR

# 1. QÃ«llimi
Ky skript ka pÃ«r qÃ«llim identifikimin e rreshtave qÃ« pÃ«rmbajnÃ« vlera statistikisht tÃ« jashtÃ«zakonshme, tÃ« cilat duhet tÃ« pÃ«rjashtohen pÃ«r tÃ« garantuar saktÃ«si tÃ« mÃ«tejshme nÃ« analizÃ«. Detektimi i pÃ«rjashtuesve (outliers) bÃ«het duke pÃ«rdorur metodÃ«n IQR (Interquartile Range) dhe ruan rezultatin nÃ« njÃ« CSV tÃ« ri duke shtuar kolonÃ«n is_outlier pÃ«r secilÃ«n rresht.

## PÃ«rmbajtja
- VarÃ«sitÃ« (Dependencies)
- Struktura e skedarÃ«ve dhe rrugÃ«t (Paths)
- PÃ«rshkrimi i plotÃ« i skriptit
- Leximi i dataset-it
- PÃ«rzgjedhja e kolonave numerike
- Funksioni detect_outliers_iqr
- Shtimi i kolonÃ«s is_outlier
- Krijimi i folderit tÃ« output-it
- Ruajtja e dataset-it tÃ« pÃ«rditÃ«suar
- Mesazhet nÃ« konzol
- Si ta ekzekutoni skriptin

---

# 2. VarÃ«sitÃ« (Dependencies)

PÃ«r ekzekutimin e kÃ«tij skripti nevojiten paketat:

- Python 3.x
- pandas
- numpy
- pathlib *(standard library e Python-it)*

Ky seksion shpjegon se pse kÃ«to biblioteka janÃ« tÃ« nevojshme dhe si funksionon secila nÃ« kontekst tÃ« detektimit tÃ« outliers.  
Pandas Ã«shtÃ« e domosdoshme sepse mundÃ«son manipulimin e dataset-it, leximin e CSV-ve, filtrimin dhe krijimin e kolonave tÃ« reja.  
Numpy pÃ«rdoret pÃ«r identifikimin e kolonave numerike dhe pÃ«r operacione matematikore tÃ« brendshme qÃ« pandas mbÃ«shtetet.  
Pathlib siguron njÃ« mÃ«nyrÃ« moderne dhe tÃ« sigurt pÃ«r tÃ« menaxhuar rrugÃ«t e skedarÃ«ve, duke shmangur probleme tÃ« zakonshme tÃ« sintaksÃ«s nÃ« rrugÃ«t string.

### Instalimi i paketave:
```bash
pip install pandas numpy
```

---

# 3. Struktura e skedarÃ«ve dhe rrugÃ«t (Paths)

```python
from pathlib import Path

INPUT_PATH = Path("dataset/phase1/4_Binarization_Normalization/final_binarization_file.csv")
OUTPUT_PATH = Path("dataset/phase2/1_Excluder_Detection/dataset_with_exclude_detection.csv")
```

Ky seksion pÃ«rshkruan pse Ã«shtÃ« zgjedhur strukturimi nÃ« faza (phase1, phase2), njÃ« metodÃ« qÃ« pÃ«rdoret shpesh nÃ« data pipelines, ku Ã§do hap prodhon njÃ« dataset tÃ« ri tÃ« gatshÃ«m pÃ«r hapin e ardhshÃ«m.  
PÃ«rdorimi i pathlib lejon krijimin e rrugÃ«ve tÃ« sigurta qÃ« funksionojnÃ« nÃ« Ã§do sistem operativ.

---

# 4. PÃ«rshkrimi i plotÃ« i skriptit

Kodi i plotÃ« i skriptit:

```python
import pandas as pd
import numpy as np
from pathlib import Path

INPUT_PATH = Path("dataset/phase1/4_Binarization_Normalization/final_binarization_file.csv")
OUTPUT_PATH = Path("dataset/phase2/1_Excluder_Detection/dataset_with_exclude_detection.csv")

df = pd.read_csv(INPUT_PATH)
numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()

def detect_outliers_iqr(dataframe, cols):
    outlier_mask = pd.Series(False, index=dataframe.index)

    for col in cols:
        Q1 = dataframe[col].quantile(0.25)
        Q3 = dataframe[col].quantile(0.75)
        IQR = Q3 - Q1

        lower = Q1 - 1.5 * IQR
        upper = Q3 + 1.5 * IQR

        col_outliers = (dataframe[col] < lower) | (dataframe[col] > upper)
        outlier_mask |= col_outliers

    return outlier_mask

df["is_outlier"] = detect_outliers_iqr(df, numeric_cols)

OUTPUT_PATH.parent.mkdir(parents=True, exist_ok=True)
df.to_csv(OUTPUT_PATH, index=False)

print("Detektimi i pÃ«rjashtuesve u krye me sukses!")
print(f"Rezultati u ruajt nÃ«: \n {OUTPUT_PATH}")
```

Ky seksion pÃ«rshkruan hap pas hapi se Ã§farÃ« bÃ«n skripti dhe si funksionon llogjika e tij. PÃ«rfshin pÃ«rshkrime tÃ« detajuara tÃ« funksionit detect_outliers_iqr dhe pse pÃ«rdoret metoda IQR pÃ«r detektim tÃ« vlerave tÃ« jashtÃ«zakonshme.

---

# 5. Leximi i dataset-it

```python
df = pd.read_csv(INPUT_PATH)
```

Ky hap i lejon skriptit tÃ« ngarkojÃ« dataset-in nÃ« memorien e programit. Dataset-i bÃ«het gati pÃ«r analiza tÃ« mÃ«tejshme. PÃ«rshkrimi nÃ« kÃ«tÃ« seksion shpjegon se si pandas trajton CSV-tÃ« dhe si i identifikon tipet e kolonave.

---

# 6. PÃ«rzgjedhja e kolonave numerike

```python
numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
```

KÃ«tu pÃ«rzgjidhen vetÃ«m kolonat numerike sepse vetÃ«m ato janÃ« tÃ« vlefshme pÃ«r llogaritjen e IQR.  
Shpjegimi i zgjeruar tregon pse kolonat kategorike ose tekstuale nuk kanÃ« kuptim nÃ« kÃ«tÃ« analizÃ«.

---

# 7. Funksioni detect_outliers_iqr

Ky funksion pÃ«rshkruhet nÃ« detaje: si llogariten kuartilet Q1 dhe Q3, si del vlera e IQR, pse pragjet bazohen nÃ« 1.5 Ã— IQR, dhe si ndÃ«rtohet maska pÃ«r pÃ«rjashtuesit pÃ«r tÃ« gjitha kolonat numerike. Shpjegohet edhe pse kjo metodÃ« konsiderohet e qÃ«ndrueshme dhe mÃ« pak e ndikuar nga outliers sesa metodat statistikore bazuar nÃ« mesatare dhe devijim standard.

---

# 8. Shtimi i kolonÃ«s is_outlier

```python
df["is_outlier"] = detect_outliers_iqr(df, numeric_cols)
```

Ky seksion shpjegon rÃ«ndÃ«sinÃ« e kolonÃ«s sÃ« re dhe mÃ«nyrat se si mund tÃ« pÃ«rdoret nÃ« analiza tÃ« mÃ«tejshme, nga filtrimi deri te vizualizimet.

---

# 9. Krijimi i folderit tÃ« output-it

```python
OUTPUT_PATH.parent.mkdir(parents=True, exist_ok=True)
```

PÃ«rshkrimi tregon pse kjo linjÃ« Ã«shtÃ« kritike â€” shmang gabimet e ruajtjes kur folderÃ«t mungojnÃ« dhe rrit stabilitetin e skriptit.

---

# 10. Ruajtja e dataset-it tÃ« pÃ«rditÃ«suar

```python
df.to_csv(OUTPUT_PATH, index=False)
```

Kjo pjesÃ« shpjegon procesin e ruajtjes dhe arsyen pse kolona e indeksit nuk ruhet.

---

# 11. Mesazhet nÃ« konzol

```
Detektimi i pÃ«rjashtuesve u krye me sukses!
Rezultati u ruajt nÃ«:
dataset/phase2/1_Excluder_Detection/dataset_with_exclude_detection.csv
```

KÃ«to mesazhe informojnÃ« pÃ«rdoruesin qÃ« gjithÃ§ka ka shkuar siÃ§ duhet.

---

# 12. Si ta ekzekutoni skriptin

1. Sigurohu qÃ« ekziston file-i input:
```
dataset/phase1/4_Binarization_Normalization/final_binarization_file.csv
```

2. Ruaje skriptin si:
```
detect_outliers_iqr.py
```

3. Ekzekuto:
```bash
python detect_outliers_iqr.py
```

4. Output-i gjendet nÃ«:
```
dataset/phase2/1_Excluder_Detection/dataset_with_exclude_detection.csv
```
# Eliminimi i Outliers pÃ«r tÃ« Shmangur Zbulimet e Pasakta

# 1. QÃ«llimi i Skriptit
PÃ«rmes kÃ«tij skripti sigurohemi qÃ« analizat statistikore tÃ« mos deformohen nga vlerat ekstreme (outliers). Kjo arrihet duke ndarÃ« dataset-in nÃ« dy pjesÃ« tÃ« qarta: njÃ« pÃ«r analiza tÃ« sigurta dhe njÃ« pÃ«r hulumtim tÃ« vlerave tÃ« skajshme. Pra ky skript pÃ«rpunon dataset-in e fazÃ«s 2 duke ndarÃ« rreshtat pa outliers dhe vetÃ«m outliers, bazuar nÃ« kolonÃ«n is_outlier tÃ« krijuar mÃ« parÃ« nga skripti i detektimit tÃ« pÃ«rjashtuesve.

## PÃ«rmbajtja

* VarÃ«sitÃ« (Dependencies)
* RrugÃ«t e pÃ«rdorura (Paths)
* PÃ«rshkrimi i kodit
* Leximi i dataset-it
* Verifikimi i kolonÃ«s is_outlier
* Gjenerimi i filtrave pÃ«r outliers
* Ndarja e dataset-it
* Ruajtja e rezultateve
* Mesazhet informative
* Si tÃ« ekzekutohet skripti

---

## 2. VarÃ«sitÃ« (Dependencies)

Ky skript kÃ«rkon:

* Python 3.x
* pandas
* pathlib (standard library)

### Instalimi:

```
pip install pandas
```

---

## 3. RrugÃ«t e pÃ«rdorura (Paths)

KÃ«tu pÃ«rcaktohen tre rrugÃ«:

* **INPUT_PATH** â€“ dataset-i me `is_outlier`
* **OUTPUT_CLEAN_PATH** â€“ dataset pa outliers
* **OUTPUT_OUTLIERS_PATH** â€“ dataset vetÃ«m me outliers

```python
INPUT_PATH = Path("dataset/phase2/1_Excluder_Detection/dataset_with_exclude_detection.csv")
OUTPUT_CLEAN_PATH = Path("dataset/phase2/2_Avoiding_Inaccurate_Disclosures/dataset_no_outliers.csv")
OUTPUT_OUTLIERS_PATH = Path("dataset/phase2/2_Avoiding_Inaccurate_Disclosures/dataset_only_outliers.csv")
```

---

## 4. PÃ«rshkrimi i kodit

Skripti lexon dataset-in, kontrollon ekzistencÃ«n e kolonÃ«s `is_outlier`, pastaj ndan rreshtat nÃ«:

* dataset **pa outliers**, dhe
* dataset **vetÃ«m me outliers**.

NÃ« fund, i ruan tÃ« dy rezultatet nÃ« CSV tÃ« veÃ§antÃ«.

---

## 5. Leximi i dataset-it

```python
df = pd.read_csv(INPUT_PATH)
```

---

## 6. Verifikimi i kolonÃ«s `is_outlier`

NÃ«se kolona mungon, skripti ndalet.

```python
if "is_outlier" not in df.columns:
    raise ValueError("Kolona 'is_outlier' nuk u gjet nÃ« dataset...")
```

---

## 7. Gjenerimi i filtrit pÃ«r outliers

Kolona pranohet nÃ« formatet: `1`, `true`, `yes` (case-insensitive).

```python
outlier_mask = df["is_outlier"].astype(str).str.lower().isin(["1", "true", "yes"])
```

MundÃ«son fleksibilitet ndaj formateve tÃ« ndryshme.

---

## 8. Ndarja e dataset-it

```python
df_no_outliers = df[~outlier_mask].copy()
df_only_outliers = df[outlier_mask].copy()
```

---

## 9. Ruajtja e rezultateve

Folderi krijohet automatikisht nÃ«se mungon.

```python
OUTPUT_CLEAN_PATH.parent.mkdir(parents=True, exist_ok=True)

df_no_outliers.to_csv(OUTPUT_CLEAN_PATH, index=False)
df_only_outliers.to_csv(OUTPUT_OUTLIERS_PATH, index=False)
```

---

## 10. Mesazhet nÃ« konzol

Skripti printon:

* Numrin e rreshtave total
* Numrin e outliers
* Lokacionin e file-ve tÃ« ruajtura

---

## 11. Si tÃ« ekzekutohet skripti

1. Sigurohu qÃ« skripti i detektimit tÃ« outliers ka gjeneruar:

```
dataset/phase2/1_Excluder_Detection/dataset_with_exclude_detection.csv
```

2. Ruaje kÃ«tÃ« skript si:

```
avoid_inaccurate_disclosures.py
```

3. Ekzekuto:

```
python avoid_inaccurate_disclosures.py
```

4. Rezultatet do tÃ« jenÃ«:

```
dataset/phase2/2_Avoiding_Inaccurate_Disclosures/dataset_no_outliers.csv
dataset/phase2/2_Avoiding_Inaccurate_Disclosures/dataset_only_outliers.csv
```
# Explorimi i tÃ« DhÃ«nave (EDA)

---

## 1. QÃ«llimi i skriptit
- Shpjegimi i shkurtÃ«r i qÃ«llimit tÃ« eksplorimit tÃ« tÃ« dhÃ«nave
- Ã‡ka pritet tÃ« merret nga EDA?

---

## 2. VaresitÃ« (Dependencies)
- Python 3.x
- pandas
- numpy
- matplotlib
- seaborn

Instalim:
> pip install pandas numpy matplotlib seaborn

---

## 3. StrukturÃ« projektit
- dataset/
- phase2/
- 2_Avoiding_Inaccurate_Disclosures/
- 3_Data_Exploration/
    - script_data_exploration.py
    - README.md (ky dokument)

---

## 4. Ekzekutimi i skriptit
> python script_data_exploration.py  
ose  
> python.exe script_data_exploration.py

---

## 5. Ã‡ka ndodh nÃ« skript? (shkurt me pika)

### 5.1 Leximi dhe inspektimi
- Leximi i dataset_no_outliers.csv
- Shfaq 5 rreshtat e parÃ«
- Shfaq info tÃ« datasetit

### 5.2 Statistika pÃ«rmbledhÃ«se (Univariate)
- describe() pÃ«r kolonat numerike
- histogram pÃ«r amount_of_scholarship
- boxplot pÃ«r amount_of_scholarship
- histogram pÃ«r class

### 5.3 Analiza e variablave kategorikÃ«
- value_counts() pÃ«r state_name
- value_counts() category_name
- value_counts() aspirational_final
- value_counts() gender_male
- barplots pÃ«r secilÃ«n kategori (opsionale)

### 5.4 Analiza multivariate
- groupby() pÃ«r category_name dhe amount_of_scholarship (count, mean, min, max)
- groupby() aspirational_final -> mesatare
- groupby() gender_male -> mesatare
- groupby() category_name + gender_male -> mesatare
- boxplots sipas kategorive

### 5.5 Korelacion
- Convert kategori nÃ« int nÃ«se duhen
- llogarit corr()
- shfaq heatmap

---

## 6. Rezultati final
- PÃ«rmbledhje nÃ« fund e gjetjeve nga vizualizimet dhe statistikat

---

## 7. Screenshots / Grafika
- Histogram amount_of_scholarship
- <img width="796" height="431" alt="image" src="https://github.com/user-attachments/assets/8b801de9-866e-4416-825c-c0e2505c39aa" />

- Boxplot amount_of_scholarship
- <img width="521" height="528" alt="image" src="https://github.com/user-attachments/assets/9716658a-1075-4c39-9070-04b7ccaa50d6" />
- <img width="595" height="432" alt="image" src="https://github.com/user-attachments/assets/44a34cbf-e09e-4715-b8fd-d8c6b86f6a91" />
- <img width="598" height="426" alt="image" src="https://github.com/user-attachments/assets/5b188aea-6763-4d58-acf0-ec5691785fe4" />
- <img width="399" height="494" alt="image" src="https://github.com/user-attachments/assets/32b81ee6-9621-4e84-97f5-08e9424ce4de" />
- <img width="398" height="432" alt="image" src="https://github.com/user-attachments/assets/4329a131-2b98-457e-8d82-db7f0e4cc2a9" />
- <img width="798" height="529" alt="image" src="https://github.com/user-attachments/assets/e44050c1-4989-429b-8ec5-6a6a5e8e0383" />
- <img width="598" height="533" alt="image" src="https://github.com/user-attachments/assets/da905e8f-fc6e-4f71-aeb0-76f94213ed74" />
- <img width="598" height="526" alt="image" src="https://github.com/user-attachments/assets/f42b4e48-3061-4d3d-82f3-be54272cc693" />

- Barplot kategorive
- 
- Boxplot multivariate
- Heatmap korelacioni
- <img width="600" height="427" alt="image" src="https://github.com/user-attachments/assets/af3765f9-26a5-4fe5-9b1a-3aecca028c94" />

---

## Konkluzioni 
Puna e realizuar pÃ«rfshin njÃ« proces tÃ« plotÃ« dhe tÃ« strukturuar tÃ« pÃ«rgatitjes sÃ« tÃ« dhÃ«nave, duke nisur nga detektimi i vlerave tÃ« jashtÃ«zakonshme, eliminimi i tyre dhe pÃ«rfundimisht eksplorimi statistikor i dataset-it tÃ« pastruar. Fillimisht, skripti i detektimit tÃ« outliers identifikon me saktÃ«si rreshtat problematikÃ« duke pÃ«rdorur metodÃ«n e IQR dhe krijon njÃ« kolonÃ« tÃ« dedikuar pÃ«r tÃ« shÃ«nuar secilÃ«n vlerÃ« tÃ« skajshme. MÃ« pas, skripti i dytÃ« organizon dhe ndan dataset-in nÃ« dy versionet e nevojshme: <br>
â€” atÃ« tÃ« pastÃ«r dhe <br>
â€” atÃ« qÃ« pÃ«rmban vetÃ«m outliers,<br>
duke siguruar qÃ« analiza e mÃ«tejshme tÃ« jetÃ« e saktÃ« dhe pa devijime statistikore.

NÃ« fazÃ«n finale, skripti i EDA-s analizon nÃ« detaje tÃ« dhÃ«nat e filtruara, duke gjeneruar statistika pÃ«rmbledhÃ«se, vizualizime dhe analiza korelacionesh qÃ« ndihmojnÃ« nÃ« kuptimin e shpÃ«rndarjes, marrÃ«dhÃ«nieve dhe modeleve tÃ« brendshme tÃ« dataset-it. KÃ«to tre hapa sÃ« bashku krijojnÃ« njÃ« pipeline tÃ« plotÃ« dhe tÃ« mirÃ«strukturuar pÃ«r pastrimin, validimin dhe eksplorimin e tÃ« dhÃ«nave, duke garantuar njÃ« bazÃ« tÃ« fortÃ« pÃ«r analiza tÃ« avancuara, modelim apo vendimmarrje tÃ« mÃ«tejshme.

